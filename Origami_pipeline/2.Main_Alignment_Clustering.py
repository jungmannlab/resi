#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@author: Susanne Reinhardt

This "main" script initiates several steps in the analysis of the Origami picks.
Input: Path to a folder containing the hdf5 files generated by the
Seperate_Picks.py script for both imaging rounds (_oriX.hdf5 files).
Alignment sites have to be picked for each origami prior to the execution of this
script and saved in a folder called alignment_picks" 
1. Find the best parameters for a euclidian transforamtion to align the picked 
   aligment sites of both rounds.
2. Apply the transformation on the full origami picks of round 2 to overlap it 
   with round 1.
3. Cluster all datasets and obtain RESI localizations as centers of each cluster.
4. NND analysis
"""


'''
=============================================================================
User Input
=============================================================================
'''

'''Pixel Size of your data in nm.'''
px_size = 130

'''Path to the folder containing Origami hdf5 files''' 
path = r"W:\users\...\2d_test_data_RRO_1bp"


'''Filename of round 1 and round 2. This part of the filename must be common 
   to all _oriX.hdf5 files of one imaging round.'''
round1_filename_base = "R1_apicked"
round2_filename_base = "R3_apicked"


'''Choose a short name to identify each round. 
   This will be used to name output files.'''
round1_name = "R1"
round2_name = "R3"


'''Clustering Parameters'''
radius_xy = 4
radius_z = 0 # set to 0 for 2d data
N_min = 50




'''
=============================================================================
Script - No need for modifications
=============================================================================
'''


import sys
import glob
import os
import os.path
import pandas as pd
import itertools

from Functions.find_eucl_transf_f import find_eucl_transf_f
from Functions.apply_eucl_transf_f import apply_eucl_transf_f
from Functions.Clusterer_resi_f import clusterer_start
from Functions.NND_f import nearest_neighbor
#from MainFunctions.Cluster_PostProcessing_new_f import postprocessing
#from MainFunctions.Cluster_PostProcessing_new_f import postprocessing_cross



data = [[round1_name, round1_filename_base, radius_xy, N_min],
        [round2_name, round2_filename_base, radius_xy, N_min]]



'''Find euclidian transformation.'''
'''============================================================================'''
# This code expects two different channels!
if len(data) != 2:
    raise Exception("There must be data from exactly two channels!")

# Before running the code for finding the transformation, check if the output file
# eucl_transf_data.xlsx already exists from a previous run of the code.
eucl_transf_data = os.path.join(path,"eucl_transf/eucl_transf_data.xlsx")
if os.path.isfile(eucl_transf_data) != True: 
    print("Find best Euclidian transformation for channel alignment")
    # The transformation will be performed on the R4 sites in both rounds
    # Round 1 = Channel R1, Round 2 = Channel R3
    
    path_alignment_picks = os.path.join(path,"alignment_picks")

    if not os.path.isdir(path_alignment_picks):
            raise Exception("The folder containing the alignment picks was not found. Check if it is named 'alignment_picks'.")

    ch13_files = glob.glob(os.path.join(path_alignment_picks, "*.hdf5"))
    ch1_files = sorted(file for file in ch13_files if data[0][1] in file)

    # get the respective list for the ch3 files. 
    # Instead of extracting it in the same way from the ch13_files list
    # it will be created from the ch1_files list. If it would be extracted
    # from ch3_files, we might not notice if files in ch3 are missing.... 
    ch3_files = []
    for ch1_file in ch1_files:
        ch3_file = ch1_file.replace(data[0][1], data[1][1])
        ch3_files.append(ch3_file)


    if ch1_files == [] or ch3_files == []:
        sys.exit("No files in channel 1 and/ore channel 2 were found.")
    
    # Run the function to get the transformation parameters
    find_eucl_transf_f(path, ch1_files, ch3_files, px_size)

else:
    print("Euclidian transformation for channel alignment has already been determined.")



'''Apply euclidian transformation to R3 channel.'''
'''============================================================================'''

# Check if the eucl_transf_data.xlsx file has been generated previously
eucl_transf_data = os.path.join(path,"eucl_transf/eucl_transf_data.xlsx")
if os.path.isfile(eucl_transf_data) == True:
    ch13_files = glob.glob(os.path.join(path, "*.hdf5"))
    ch1_files = sorted(file for file in ch13_files if data[0][1] in file and "ori" in file and "ClusterD" not in file and "_resi_" not in file)

    ch3_files = []
    for ch1_file in ch1_files:
        ch3_file = ch1_file.replace(data[0][1], data[1][1])
        ch3_files.append(ch3_file)

    # Check if the alignment has already been performed beforehand
    check_aligned_file = os.path.split(ch3_files[-1])[1]
    check_aligned_file = os.path.split(ch3_files[-1])[0] + "/" + check_aligned_file[:-5] + "_aligned.hdf5"
    if os.path.isfile(check_aligned_file) != True:
        # Perform the alignment of the complete origami picks
        print(ch1_files)
        apply_eucl_transf_f(path, ch1_files, ch3_files, px_size)

else:
    raise Exception("Euclidian transformation for channel alignment has not yet been determined.")



'''Perform Clustering'''
'''============================================================================'''

def Clusterer_check(path, radius, min_cluster_size, filename_base, i):
    Clusterer_filename_extension = "_ClusterD"+str(radius)+"_"+str(min_cluster_size)
    for file in glob.glob(os.path.join(path, "*.hdf5")): # searches all hdf5 files
       
        # check if file is part of the dataset to be clustered (filename base in file)
        # The strings "picked" and "ori" must be part of the filename.
        # check that the file itself is not a file produced by the clusterer itself 
            # (file does not contain ClusterD6_15 etc.)       
        # check that file is not an output file that was produces in a previous
        # run of this script. Like files containing _resi_
        
        if (
                filename_base in file and 
                "picked" in file and
                "ori" in file and
                Clusterer_filename_extension not in file and 
                "ClusterD" not in file and
                "coupling" not in file and 
                "_resi_" not in file
            ): 
            
            if (i == 0) or (i == 1 and "_aligned" in file):
                # check that the file has not been clustered already
                npz_file = file[:-5] + "_varsD" + str(radius) + "_" + str(min_cluster_size) + ".npz"
                if radius_z != 0:
                    npz_file = file[:-5] + "_varsD" + str(radius) + "_" + str(min_cluster_size) + "_" + str(radius_z) + ".npz"
                if os.path.isfile(npz_file) != True: 
                    # Run the SMLM Clusterer and calculate RESI locs
                    clusterer_start(file, radius, min_cluster_size, px_size, radius_z)
                

               
'''Clusterer'''

for i in range(len(data)):
    protein_info = data[i]
    radius = protein_info[2]
    min_cluster_size = protein_info[3]
    filename_base = protein_info[1]
        
    Clusterer_check(path, radius, min_cluster_size, filename_base, i)



'''Postprocessing'''
'''============================================================================'''

for protein_info in data:
    protein = protein_info[0]
    radius = protein_info[2]
    min_cluster_size = protein_info[3]
    filename_base = protein_info[1]
    
    
    RESI_filename_extension = "_resi_"+str(radius)+"_"+str(min_cluster_size)
    for file_resi in glob.glob(os.path.join(path, "*.hdf5")): # searches all hdf5 files
       if filename_base in file_resi and RESI_filename_extension in file_resi and 'info.' not in file_resi: #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
           postprocess_hNN_file = os.path.split(file_resi)[0] + "/AdditionalOutputs/" + os.path.split(file_resi)[1] + "_higher_Neighbors_" + protein + ".csv"
           if os.path.isfile(postprocess_hNN_file) != True:
               nearest_neighbor(protein, file_resi, protein, file_resi, px_size)



# cross NND
if len(data) > 1:
    for pair in itertools.combinations(data,2): # pair = all combinations of proteins, each combination stored as a tuple of two elements in the data list

        protein1 = pair[0][0]
        radius1 = pair[0][2]
        min_cluster_size1 = pair[0][3]
        filename_base1 = pair[0][1]
        
        protein2 = pair[1][0]
        radius2 = pair[1][2]
        min_cluster_size2 = pair[1][3]
        filename_base2 = pair[1][1]
        
        # data 1 is from filename_base1 and the corresponding data2 file will be searched automatically
        RESI_filename_extension1 = "_resi_"+str(radius1)+"_"+str(min_cluster_size1)
        for file_resi1 in glob.glob(os.path.join(path, "*.hdf5")): # searches all hdf5 files
           if filename_base1 in file_resi1 and RESI_filename_extension1 in file_resi1 and 'info.' not in file_resi1: #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                file_resi2 = file_resi1.replace(filename_base1, filename_base2)
                file_resi2 = file_resi2.replace("_resi_"+str(radius1)+"_"+str(min_cluster_size1), "_aligned_resi_"+str(radius2)+"_"+str(min_cluster_size2))
        
                postprocess_hNN_file_ex_1 = os.path.split(file_resi1)[0] + "/AdditionalOutputs/" + os.path.split(file_resi1)[1] + "_higher_Neighbors_" + protein1 + "_to_" + protein2 + ".csv"
                postprocess_hNN_file_ex_2 = os.path.split(file_resi2)[0] + "/AdditionalOutputs/" + os.path.split(file_resi2)[1] + "_higher_Neighbors_" + protein2 + "_to_" + protein1 + ".csv"

                if os.path.isfile(postprocess_hNN_file_ex_1) != True or os.path.isfile(postprocess_hNN_file_ex_2) != True:
                    nearest_neighbor(protein1, file_resi1, protein2, file_resi2, px_size)
                    nearest_neighbor(protein2, file_resi2, protein1, file_resi1, px_size)
                    
